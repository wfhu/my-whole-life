#### ES集群深入数据有效性进行优化

时间：2019年1月-2019年3月

lbs索引里面，province和city为0的记录是没有任何意义的，可以清理掉。

active/online索引里面，timestamp超过一定范围的，都是没有意义的数据，可以清理掉。

另外，修改业务逻辑，避免产生新的无效数据。

效果一：lbs数据总共约100亿条，清理无效数据31.5亿条，占比30%。

---

#### 广播缓存历史非活跃uid数据清理

时间：2019年2月-2019年3月

广播缓存占用内存空间持续增长，使用的是redis的set结构，只增不减，但是实际上里面有很多是不活跃的僵尸用户，合理地清理掉这些用户，对于节约内存空间、提高查询效率都非常有帮助。

---

#### CouchBase热点数据导致集群踢出节点

时间：2019年1月-2019年2月

现象：一个由8个节点组成的CouchBase集群，发现总是频繁地出现某一节点突然被自动踢出集群，然后另一机器CPU负载超高的现象，导致整个集群响应非常慢。一段时间内改现象反复出现。

排查：替换过机器、重建过集群，问题还是重复出现。基本排除硬件问题。

定位：发现是一个用户针对某个uid（在CouchBase中是key）做压力测试，该uid是虚拟uid，峰值达到3w-4w/s的写入量，导致某个节点过热，从而被踢出集群。第一个节点被提出集群后，由于副本集设置为1，CouchBase的安全机制确保不会再自动踢掉第二个节点，所以导致另一个节点CPU占用超高，进而导致整个集群几乎不可用。

解决：在业务逻辑处做判断，当请求量达到一定程度时，做服务降级处理，丢弃部分数据。

---

#### 某ES集群优化

时间：2019年3月

现象：4台机器组成的ES集群，业务侧反馈**查询非常慢**，但是看机器监控压力都很小。

查找和解决：客户端连接的线程池设置为200，而我们的服务器是16核的，过大的线程池数量导致查询效率低下。通过修改线程池大小，线程数不超过25： 查询不要超过25，写入不要超过 16。业务侧做了上述修改之后，再无类似的问题出现。

---

#### 基于KVM和OpenStack下虚拟机CPU超分导致的性能问题

时间：2019年7月

OpenStack的多个VM，两倍多地超分了CPU资源，即使某一个VM的CPU是空闲的，也会引起整个物理机上其他VM的CPU出现steal的情况。把空闲的VM所分配的CPU数量降下去之后，其他VM的CPU性能也有了明显的改善。

整个物理机是48个核心（实际上只有24个物理核，开启HT技术后显示有48核），所有VM整体超分到60+虚拟CPU，其中有一个MySQL的备库分了16个vCPU，但是实际压力很小；另外一个MQ的虚拟机是8核，但是出现了超过16%的steal。最开始查找问题的时候，并没有怀疑到这个MySQL的备库（因为它本身的压力确实非常小），查了一圈之后，尝试将这个备库关机，发现MQ的性能一下子就好了。

---

#### PIka和Sentinel性能问题一例

时间：2019年8月

Pika是5主5从，每个实例大约600G数据量以及大约150亿的数据量。

现象是：访问Pika的代理大量报超时（10s超时时长）

怀疑一：SSD磁盘写放大导致的性能下降。经排查是有可能的，但是不会突然出现，初步否定。

怀疑二：业务量突然增长。经排查否定。

最终，通过查看Pika的slowlog发现了很多INFO命令超时（20ms），很多都在200ms-700ms之间。

再查看延迟发生的时间点，发现那个时间添加了5台sentinel，初步怀疑是Pika的INFO指令效率不高，导致sentinel的INFO指令消耗了太多资源导致。

解决方案：将5台sentinel减少到3台，观察一段时间，确实没有类似的报警再出现了。

---

通过Lua脚本避免对同一个key expire多次

时间：2019年8月



