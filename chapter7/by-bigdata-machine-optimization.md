## 【优化】博雅-大数据机器高故障率分析和优化

---

时间：2014年-2015年左右

关键字：博雅，大数据，IDC机房，散热，监控，HDFS，磁盘，高IO

说明：本文是2018年12月开始写的，离事件发生已经有四年左右了，细节已经记不清楚了，主要是记录处理问题的思路以及从中吸取的教训。

重点概括：

* 通过优化服务器运行的环境，提升散热效率，降低服务器故障率。
* 通过优化软件运行的方式，降低磁盘读写压力，降低服务器故障率。

---

### 背景交代

当时博雅有一套大数据集群，大概3个机柜，30-40台机器，全是统一采购的、全新的DELL的机器，配置比较高（32核还是48核，128G内存，2T SATAx12 + 600 SAS x 2）。在集群正式启用一段时间后，开始频繁地出现硬盘故障，因为设备还在质保期内，我们也只能频繁地联系供应商上门更换硬件，并坚定地怀疑这批硬盘的品质有问题。

虽然没有带来成本上的问题，也没有给业务带来太大的影响，但这样下去终究不是长久之计，我们开始尝试深入研究其中的原因并尝试做长久的优化工作。

---

**机房环境分析**

首先我们去了机房做了一番考察，发现这一批机器是好几台共用一个导轨或者托盘，堆叠可以达到四五层，没有每台机器单独使用导轨或者托盘，导致部分机器之间的散热可能存在问题。

我们制定并实施了优化方案，使每台机器之间都空了三分之二个U位，同时也把以前缺少的机房环境温度、磁盘温度等基础环境监控加上了。

虽然不能确定故障率高是否和散热有直接关系，但这确实是一个不规范的点，无论如何也是需要优化的。

**应用使用情况分析**

我们对机器的使用情况做了详细的分析，发现即使在业务低峰期（白天），磁盘IO的使用率也非常高，发现总有du -sh的命令在运行。排查了很久，发现当时hadoop HFDS的文档里明确有写，是会定期做"du -sh"操作，来收集磁盘剩余空间等信息的。

我们把这块自动的操作给修改关掉了，改成通过代码来定期运行，效果非常明显，磁盘IO使用率下降至正常水平。

---

在做了这两个操作之后，磁盘故障率明显地下降了，基本上半年才坏6块左右，而以前基本上每个星期都能坏好几块。

**总结**

1. 基础设施需要非常规范，不能太过于随意，小的问题累加起来就是大问题了。
2. 对使用的软件要有比较深入的熟悉，对默认配置要有清晰、全面的认识，不能停留在搭建好、用起来的水平。
3. 需要有最基础的监控和预警系统，可以从侧面发现一些异常状况，不能不管不顾。



